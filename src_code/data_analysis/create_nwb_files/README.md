# Overview

This folder stores the code to create .nwb files that contain the preprocessed neural and behavioral data associated with Papadopoulos et al (2025).
"Modulation of metastable ensemble dynamics explains the inverted-U relationship between tone discriminability and arousal in auditory cortex"  

For each session, this code creates a .nwb file from a pre-existing .h5 file that contains the preprocessed neural and behavioral data for that session.  

The .h5 files should be generated ahead of time using the code in `src_code/data_analysis/create_processed_dataset/`.

The generated .nwb files can then be used to publicly distribute the data via the Dandi Archive. All data analysis scripts can also take the .nwb files as input.

# Notes

Santiago wrote the original code for creating the .nwb files. That code code is located here:  
`https://github.com/sjara/uobrainflex_papers/tree/main/papadopoulos2024`  

The above git repository is also cloned to my account on the picard sever, and can be accessed at:  
`/home/liap/PostdocWork_Oregon/My_Projects/uobrainflex_papers/papadopoulos2024`

I adapated Santiago's code slightly for our specific purposes. My version of the code is explained below.

# Usage

This directory contains three files.  

1. `nwb_metadata_mainCellSelection.py`: Contains the metadata for each session (stored in dictionaries) that is needed to create the .nwb files (e.g., subject age, experimenter, session name, etc). This metadata is setup to generate .nwb files using the cell selection and pupil normalization criteria that were used for the figures in the main text of the manuscript. In principle, one could create other metadata files for different versions of the preprocessing.

2. `convert_h5_to_nwb.py`: Creates .nwb files from the .h5 files containing preprocessed electrophysiology and pupil/running data.

3. `example_read_nwb.py`: Example showing how to read the resulting .nwb files. See also `../fcn_processedNWBdata_to_dict.py`.  

The .nwb files for each session are stored on the picard cluster at:  
`/mnt/data0/liap/PostdocWork_Oregon/My_Projects/PROJ_VariabilityGainMod/data_files/analysis_SuData/processed_data_LP/nwb/`

# Uploading to DANDI Archive

After generating the .nwb files, we packaged them in a dandiset to publicly distribute the data. To do this, we followed the steps on the DANDI documentation page.  

`https://docs.dandiarchive.org/user-guide-sharing/uploading-data/`

Major steps:  

1. Created a dandi account. My account is linked to my github (lia-papadopoulos).
2. Created a dandiset page online to store the data by following the instructions at: `https://docs.dandiarchive.org/user-guide-sharing/creating-dandiset/`. Our dandiset is located at: `https://dandiarchive.org/dandiset/000986` .
3. Installed `DANDI` client on picard in a dedicated conda environment called `dandi`.
4. Converted data we want to share to .nwb format using the scripts detailed above.
5. Validated .nwb files using `pynwb-validate`. See `https://docs.dandiarchive.org/user-guide-sharing/validating-files/`.
6. Uploaded the data by following the instructions at: `https://docs.dandiarchive.org/user-guide-sharing/uploading-data/`. We used the DANDI CLI and API Key method to do this.

Source directory for dandi:
`/mnt/data0/liap/PostdocWork_Oregon/My_Projects/PROJ_VariabilityGainMod/data_files/analysis_SuData/processed_data_LP/nwb/`  

Folder generated by dandi containing the dataset to upload to the dandi archive:
`/mnt/data0/liap/PostdocWork_Oregon/My_Projects/PROJ_VariabilityGainMod/data_files/analysis_SuData/processed_data_LP/dandiset/000986/`

7. Updated all metadata on the Dandi Archive and published dataset.


